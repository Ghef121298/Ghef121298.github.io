I"#<h1 id="doremi---data-collection-lifecycle-document">DoReMi - Data Collection Lifecycle Document</h1>

<p>This documents the lifecycle of the DOREMI dataset. DOReMi is a dataset that contains data related to OMR research. DOReMi is a product of collaborative work between Steinberg (Dorico team) and QMUL (Elona Shatri and George Fazekas). This dataset aims to fill in the existing gaps in OMR research. It is part of a larger research question, whether deep learning can assist theOMR field.  What sets DOReMi aside from the other datasets is the richness of data. Given that this dataset is generated using Dorico (a music notation software from Steinberg), we had the chance to grab more musical information on it, but also different types of data. Using Dorico we can generate 5 types of data that could possibly be used in different steps of OMR. These data are images of scores (binary or colour), the musicXML file, XML files with metadata such as bounding boxes of each object together with musical information, which will be explained in detail later on, and as part of our dataset we can utilize the midi audio file which can be later used for transcription as well.</p>

<p>Music used to generate this dataset comes from a software test set provided by the Dorico team 1 . This test set includes a wider number of objects, classes and various cases of notations not normally seen in real-world music. About half of this dataset is copyright protected; therefore, we only include openly distributable scores in the final published dataset. However, pre-trained models trained in the whole dataset will also be published. DoReMi includes around 6432 images of sheet
music with nearly a million annotated objects which is 1 th 50 the size of DeepScores and 42 times the size of Muscima++. Each object on the page is annotated with category labels from 94 different classes. However, there is an emphasised class imbalance; stems and noteheads make up half of the annotated objects in the dataset. We also provide prepared subsets fulfilling different requirements on the number of pages, number of classes and the number of staves. Most of the images include one system per page; depending on the number of voices, they will have one or more staves per page.</p>

<h2 id="discovery-and-planning"><a href="#discovery-and-planning">Discovery and Planning</a></h2>

<h3 id="data-type-and-format"><a href="#Data-type-and-format">Data type and format</a></h3>
<ul>
  <li>Dorico project</li>
  <li><a href="#MusicXML">MusicXML</a></li>
  <li><a href="#OMR-metadata">OMR metadata</a></li>
  <li><a href="#Images-of-scores">Images of scores</a></li>
  <li><a href="#MIDI">MIDI</a></li>
  <li><a href="">Combining DOReMi with Muscima++ and Deepscores</a></li>
</ul>

<h2 id="initial-data-collection"><a href="">Initial Data Collection</a></h2>
<h2 id="data-preparation-and-analysis"><a href="">Data Preparation and analysis</a></h2>
<h2 id="publication-and-sharing"><a href="">Publication and Sharing</a></h2>
<h2 id="long-term-management"><a href="">Long-term Management</a></h2>

<h2 id="-discovery-and-planning-"><a href="#discovery-and-planning"> Discovery and Planning </a></h2>

<p>In this section, I will explain the data types and formats we have decided to use.</p>

<h3 id="-data-type-and-format-"><a href="#Data-type-and-format"> Data type and format </a></h3>

<p>This dataset has the following six types of data:</p>

<ul>
  <li><a href="#dorico-project">Dorico project</a></li>
  <li><a href="#MusicXML">MusicXML</a></li>
  <li><a href="#OMR-metadata">OMR metadata</a></li>
  <li><a href="#Images-of-scores">Images of scores</a></li>
  <li><a href="#MIDI">MIDI</a></li>
  <li><a href="#MEI">Music Encoding Initiative (MEI)</a></li>
</ul>

<p>The following respective data formats:</p>
<ul>
  <li>Dorico</li>
  <li>MusicXML</li>
  <li>XML</li>
  <li>PNG</li>
  <li>MIDI</li>
  <li>MEI</li>
</ul>

<p>Let’s start by explaining each one of the data types and what they contain:</p>

<p><a href="#dorico-project"> Dorico project </a>
<img src="/assets/img/Dorico_Logo.png" alt="Dorico-project" /></p>

<p>When saving the scores in Dorico, it uses the .dorico file format, which is the project type. Saving the projects is really important for the lifecycle since it allows us to use the same layout settings if we later want to generate other data or change.</p>

<p><a href="#MusicXML">  MusicXML </a></p>

<p>MusicXML is widely known and used data format in music notation which uses the XML file format. MusicXML allows interchangeability between different scorewriters. Dorico can export the projects to MusicXML. The extension for these files is .xml. 
More on how these files can be exported check <a href="https://steinberg.help/dorico_pro/v3/en/dorico/topics/project_file_handling/project_file_handling_musicxml_files_exporting_t.html"> here </a>
<br />
<br /></p>

<p><img src="/assets/img/musicXML.png" alt="MusicXML-example" /></p>

<p><a href="#OMR-metadata"> OMR metadata </a></p>

<p>Following the organisation of Muscima++, DoReMi has an OMR metadata file which includes bounding boxes of each element: top, left, width and height. It also includes the pixel mask for each element giving each object’s pixels inside the bounding box. Additionally, DoReMi provides the relationships between primitives. It vaguely follows the Music Notation Graph (MUNG), which creates a graph representation of music notations. Inlinks and outlinks reference back and forth to the ID of the objects they are related to—for instance, a notehead half outlinks to a stem or a slur or both of them. Conversely, the stem inlinks to the notehead half.</p>

<p>OMR metadata are the files that incorporate most of the information needed for OMR practitces. These files include visual information, coordinates, bounding boxes, pixel information and furthermore musical information such as duration beats, onset beats, pitch octave, midi pitch note, normalized pitch step and the dorico event id depending on the class of the object.</p>

<p>Depending on the task the dataset includes both parsed and unparsed XML files. If the dataset is to be used only for object detection or instance segmentation the parsed files are divided by page, corresponding to an image respective to the actual page. These files hold information on the bounding boxes and pixel mask information.</p>

<p><img src="/assets/img/unparsed-xml.png" alt="unparsed-xml" /></p>

<p>Certain (playable) objects are also annotated with a Dorico event ID which is a unique event identifier that provides additional information on how some objects are linked. For instance, notes like noteheads have information on the duration beats, onset beats, pitch octave, midi pitch code, normalised pitch step and an event ID. 
<br />
<br />
 <img src="/assets/img/notehead.png" alt="notehead" /></p>

<p>For elements such as clefs, our dataset provides an event ID, clef type, clef hotspot, clef required stave lines and clef stave position. Clef hotspot identifies the midi pitch that clef denotes, i.e. for treble clef is G4, as that is the pitch of the second stave line from the bottom. Clef required stave lines shows how many stave lines the clef needs.</p>

<p><img src="/assets/img/clef.png" alt="clef" /></p>

<p>Time signatures will include the event ID, time signature description, for example, 3/2 (h, 1+1+1).
<br /></p>

<p><img src="/assets/img/timesig.png" alt="timesig" /></p>

<p>Flags, if they are part of grace notes, will have a boolean value set to True. Slurs and ties have their event IDs, while barlines, rests, accidentals, augmentation dots, stems do not have such information. Beams will not have their event ID; instead, they will have a list of the event IDs their respective noteheads have.</p>

<p>Time signatures</p>

<pre>
  <code class="XML">
        <Node>
        <Id>511</Id>
        <ClassName>timeSig4</ClassName>
        <Top>467</Top>
        <Left>828</Left>
        <Width>36</Width>   
        <Height>42</Height>
        <Mask>0:11 1:16 0:20 1:16 0:20 1:15 
        ...
        0:21 1:15 0:21 1:14 0:22 1:13 0:22 
        <Data>
        <DataItem key="dorico_event_id" type="int">-1</DataItem>
        <DataItem key="time_signature_description" type="str" />
        </Data>
        &lt;/Node&gt;
  &lt;/code&gt;
&lt;/pre&gt;


![staffline](/assets/img/staff-line.png)



<a href="#Images-of-scores"> OMR metadata </a> 

Given that each project can have one or more pages, we use the mono png export from Dorico at 300 DPI. Each image is named after the OMR metadata xml file, with the number suffix, starting 001. Images are of dimension 2475 × 3504 pixels and are binarised. An example of an image is given below. 

<a href="#MIDI"> MIDI </a> 

MIDI files are exported directly from Dorico. 


<a href="#MEI"> Music Encoding Initiative (MEI) </a>


## <a href="#combining"> Combining DOReMi with Muscima++ and Deepscores </a>





[back](./)
</Mask></Node></code></pre>
:ET