<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>The 6th International Workshop on Reading Music Systems 2024</title>
    <link rel="stylesheet" href="assets/css/styles.css">
</head>
<body>
    <header>
        <h1>Elona Shatri</h1>
        <p>AI Researcher | Ethics Advocate | Music Technologist</p>
    </header>
    <nav>
        <ul>
            <li><a href="index.html">Home</a></li>
            <li><a href="about.html">About</a></li>
            <li><a href="teaching.html">Teaching</a></li>
            <li><a href="research.html">Research</a></li>
            <li><a href="projects.html">Projects</a></li>
            <li><a href="blogposts.html">Blogposts</a></li>
            <li><a href="contact.html">Contact</a></li>
        </ul>
    </nav>
    <main>
        <section id="blogpost">
            <h2>The 6th International Workshop on Reading Music Systems 2024</h2>
            <article>
                <p>
                    The 6th International Workshop on Reading Music Systems (WoRMS) took place virtually, bringing together researchers and practitioners from across the globe to discuss the latest developments in Optical Music Recognition (OMR). This year's workshop featured three paper sessions, panels, and an insightful keynote by David Rizo, providing a comprehensive exploration of the state of OMR research and its real-world applications.
                </p>
                <h3>Keynote: Beyond Optical Music Recognition</h3>
                <p>
                    David Rizo from the University of Alicante delivered the keynote, "Beyond Optical Music Recognition," reflecting on the recent advancements in OMR driven by deep learning techniques. He questioned whether the current research trajectory is adequate to fully address the complexities of OMR and explored the broader implications for creating end-user-centric systems. His presentation dissected the term "OMR" through case studies, providing a critical assessment of its evolution and future.
                </p>
                <h3>Highlights from the Paper Sessions</h3>
                <h4>Session 1: Advancing OMR Models and Analysis</h4>
                <ul>
                    <li><strong>Can Multimodal Large Language Models Read Music Score Images?</strong> — Jorge Calvo-Zaragoza and collaborators explored the potential of multimodal language models in OMR, discussing their performance on music score images.</li>
                    <li><strong>Sheet Music Transformer: End-to-End Full-Page OMR for Pianoform Sheet Music</strong> — Antonio Ríos-Vila and colleagues presented a Transformer-based pipeline for recognizing full-page piano sheet music, emphasizing scalability and efficiency.</li>
                    <li><strong>Towards Sheet Music Information Retrieval Using Multitask Transformers</strong> — Antonio Ríos-Vila and team proposed a unified approach for sheet music retrieval, combining multiple OMR tasks into a multitask transformer framework.</li>
                    <li><strong>Semantic Reconstruction of Sheet Music with Graph-Neural Networks</strong> — Guillaume de Lambertye and Alexander Pacha demonstrated how graph neural networks could semantically reconstruct sheet music, enabling more accurate music representation.</li>
                    <li><strong>Staff Layout Analysis Using the YOLO Platform</strong> — Vojtěch Dvořák and collaborators presented a YOLO-based method for analyzing staff layouts, optimizing structural recognition in OMR.</li>
                </ul>

                <h4>Session 2: Expanding OMR Applications</h4>
                <ul>
                    <li><strong>On Designing a Representation for the Evaluation of OMR Systems</strong> — Pau Torras and colleagues proposed a novel representation for evaluating OMR performance, emphasizing its utility in benchmarking.</li>
                    <li><strong>Enhanced User-Machine Interaction for Historical Sheet Music Retrieval</strong> — A. Menárguez Box and team introduced a user-machine interaction model for historical sheet music, leveraging musical notation for enhanced retrieval.</li>
                    <li><strong>Enhancing Recognition of Historical Musical Pieces with Synthetic and Composed Images</strong> — M. Villarreal Ruiz and J. A. Sánchez explored the use of synthetic images to improve recognition accuracy for historical scores.</li>
                    <li><strong>The CollabScore Project: From Optical Recognition to Multimodal Music Sources</strong> — Benoît Couasnon and collaborators discussed integrating optical and multimodal music sources into a unified platform for music analysis.</li>
                    <li><strong>Semi-Automatic Annotation of Chinese Suzipu Notation</strong> — Tristan Repolusk and Eduardo Veas presented a component-based approach for annotating Suzipu notation, addressing unique challenges in Chinese music scores.</li>
                </ul>

                <h4>Session 3: Historical and Handwritten Music</h4>
                <ul>
                    <li><strong>OMR on Early Music Sources at the Bavarian State Library with MuRET</strong> — J. Umbreit and S. Schumann discussed automating and scaling OMR for early music sources using MuRET.</li>
                    <li><strong>OMMR4all Revisited: A Semiautomatic Online Editor for Medieval Music Notations</strong> — Andreas Hartelt and Frank Puppe revisited their online editor for medieval music notations, enhancing usability and automation.</li>
                    <li><strong>Enhancing Handwritten Music Sheet Datasets Using Generative Adversarial Networks (GANs)</strong> — K. R. Palavala and collaborators demonstrated how GANs could enrich handwritten music datasets for better OMR training.</li>
                    <li><strong>Crafting Handwritten Notations: Towards Sheet Music Generation</strong> — N. Tirupati and team explored generative methods for creating handwritten sheet music, pushing the boundaries of synthetic data generation.</li>
                </ul>
            </article>
        </section>
    </main>
    <footer>
        <p>© 2024 Elona Shatri. All rights reserved.</p>
    </footer>
</body>
</html>
