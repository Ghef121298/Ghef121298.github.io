<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>The 3rd International Workshop on Reading Music Systems 2021</title>
    <link rel="stylesheet" href="assets/css/styles.css">
</head>
<body>
    <header>
        <h1>Elona Shatri</h1>
        <p>AI Researcher | Ethics Advocate | Music Technologist</p>
    </header>
    <nav>
        <ul>
            <li><a href="index.html">Home</a></li>
            <li><a href="about.html">About</a></li>
            <li><a href="teaching.html">Teaching</a></li>
            <li><a href="research.html">Research</a></li>
            <li><a href="projects.html">Projects</a></li>
            <li><a href="blogposts.html">Blogposts</a></li>
            <li><a href="contact.html">Contact</a></li>
        </ul>
    </nav>
    <main>
        <section id="blogpost">
            <h2>The 3rd International Workshop on Reading Music Systems 2021</h2>
            <article>
                <p>
                    The third edition of the International Workshop on Reading Music Systems (WoRMS) was held in a hybrid live/virtual setting on Friday, 23rd of July 2021. It brought together researchers and industry professionals working in Optical Music Recognition (OMR). This edition featured 11 papers on a variety of topics and an 
                    <a href="https://drive.google.com/file/d/1IDgOaW8tGxJt9Top7x7GSZXLsAo-XVcb/view" target="_blank">outstanding keynote from Anthony Wilkes (Organum Ltd)</a> discussing the design of ReadScoreLib.
                </p>
                <img src="assets/img/worms2021.png" alt="WoRMS 2021">
                <p>Below are summaries of some of the presented papers:</p>

                <h3><a href="https://drive.google.com/file/d/17BdTUfU6Fk8qyrpxo6L-BGTqIhLvStL5/view" target="_blank">Hybrid Annotation Systems for Music Transcription</a></h3>
                <p>
                    This paper explores combining human annotation and automated methods for music transcription. Results showed that audio extracts enhanced performance, particularly for short music segments.
                </p>

                <h3><a href="https://drive.google.com/file/d/17Dp9gIjQPZVwSFJzKK8QA6Xjcgv894wj/view" target="_blank">Neural Network for Handwritten Melodies</a></h3>
                <p>
                    Aimed at digital archiving for the University Library of Regensburg, this paper evaluates neural networks for recognising handwritten monophonic scores. Challenges include limited annotated data.
                </p>

                <h3><a href="https://arxiv.org/abs/2107.07786" target="_blank">DoReMi: A Universal OMR Dataset</a></h3>
                <p>
                    Our work presents DoReMi, a new dataset aimed at addressing challenges in OMR. We demonstrated baseline experiments using Faster R-CNN models for object detection.
                </p>

                <h3><a href="https://drive.google.com/file/d/10uUCaORERAzD-ISSm6FUeNNOOzrTRDzF/view" target="_blank">Reconstructing Digits in Music Scores</a></h3>
                <p>
                    This research highlights challenges in recognising and reconstructing digits in sheet music, achieving limited accuracy on real-world scores despite using deep learning.
                </p>

                <h3><a href="https://drive.google.com/file/d/1uSIrbiLrx1RfXEV86STS7XRuwJoa34O7/view" target="_blank">Detecting Staves and Measures with Deep Learning</a></h3>
                <p>
                    This paper investigates strategies to detect structural elements in music scores. Faster R-CNNs performed best for detecting system measures and staves.
                </p>

                <h3><a href="https://drive.google.com/file/d/1ZBRaOwsTkdOUo6sfm9xdQPuPMNyM89ho/view" target="_blank">Unsupervised Neural Document Analysis</a></h3>
                <p>
                    This study proposes combining Domain Adaptation with Selectional Auto-Encoders for unsupervised document analysis, slightly improving state-of-the-art results.
                </p>

                <h3><a href="https://drive.google.com/file/d/1ZDlU0WDmqC4-37s2gkCf2nOAt4Z5Ow-S/view" target="_blank">Multimodal Audio and Image Music Transcription</a></h3>
                <p>
                    This proof-of-concept explores combining OMR and AMT predictions, demonstrating slight improvements in error rates for monophonic scores.
                </p>

                <h3><a href="https://drive.google.com/file/d/1o4zm_fx_Fa7zclWkqgbVLx2x3DuvZidz/view" target="_blank">Sequential Next-Symbol Prediction</a></h3>
                <p>
                    This paper introduces a sequential classification-based approach to music scores using CNNs, addressing training set limitations.
                </p>

                <h3><a href="https://drive.google.com/file/d/1WAhrcPRzpuoB1fJsMkGCZamIp1CHv3c5/view" target="_blank">Agnostic Transcription and Machine Encoding</a></h3>
                <p>
                    This work focuses on encoding outputs from OMR into machine-readable formats using Machine Translation techniques.
                </p>
            </article>
        </section>
    </main>
    <footer>
        <p>Â© 2024 Elona Shatri. All rights reserved.</p>
    </footer>
</body>
</html>
