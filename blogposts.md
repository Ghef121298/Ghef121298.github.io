---
layout: post-default
---

# Blogposts and News

## The 6th International Workshop on Reading Music Systems 2024

The 6th International Workshop on Reading Music Systems (WoRMS) took place virtually, bringing together researchers and practitioners from across the globe to discuss the latest developments in Optical Music Recognition (OMR). This year's workshop featured three paper sessions, panels, and an insightful keynote by David Rizo, providing a comprehensive exploration of the state of OMR research and its real-world applications.

Keynote: Beyond Optical Music Recognition
David Rizo from the University of Alicante delivered the keynote, "Beyond Optical Music Recognition," reflecting on the recent advancements in OMR driven by deep learning techniques. He questioned whether the current research trajectory is adequate to fully address the complexities of OMR and explored the broader implications for creating end-user-centric systems. His presentation dissected the term "OMR" through case studies, providing a critical assessment of its evolution and future.

Highlights from the Paper Sessions
Session 1: Advancing OMR Models and Analysis
Can Multimodal Large Language Models Read Music Score Images?
Jorge Calvo-Zaragoza and collaborators explored the potential of multimodal language models in OMR, discussing their performance on music score images.

Sheet Music Transformer: End-to-End Full-Page OMR for Pianoform Sheet Music
Antonio Ríos-Vila and colleagues presented a Transformer-based pipeline for recognizing full-page piano sheet music, emphasizing scalability and efficiency.

Towards Sheet Music Information Retrieval Using Multitask Transformers
Antonio Ríos-Vila and team proposed a unified approach for sheet music retrieval, combining multiple OMR tasks into a multitask transformer framework.

Semantic Reconstruction of Sheet Music with Graph-Neural Networks
Guillaume de Lambertye and Alexander Pacha demonstrated how graph neural networks could semantically reconstruct sheet music, enabling more accurate music representation.

Staff Layout Analysis Using the YOLO Platform
Vojtěch Dvořák and collaborators presented a YOLO-based method for analyzing staff layouts, optimizing structural recognition in OMR.

Session 2: Expanding OMR Applications
On Designing a Representation for the Evaluation of OMR Systems
Pau Torras and colleagues proposed a novel representation for evaluating OMR performance, emphasizing its utility in benchmarking.

Enhanced User-Machine Interaction for Historical Sheet Music Retrieval
A. Menárguez Box and team introduced a user-machine interaction model for historical sheet music, leveraging musical notation for enhanced retrieval.

Enhancing Recognition of Historical Musical Pieces with Synthetic and Composed Images
M. Villarreal Ruiz and J. A. Sánchez explored the use of synthetic images to improve recognition accuracy for historical scores.

The CollabScore Project: From Optical Recognition to Multimodal Music Sources
Benoît Couasnon and collaborators discussed integrating optical and multimodal music sources into a unified platform for music analysis.

Semi-Automatic Annotation of Chinese Suzipu Notation
Tristan Repolusk and Eduardo Veas presented a component-based approach for annotating Suzipu notation, addressing unique challenges in Chinese music scores.

Session 3: Historical and Handwritten Music
OMR on Early Music Sources at the Bavarian State Library with MuRET
J. Umbreit and S. Schumann discussed automating and scaling OMR for early music sources using MuRET.

OMMR4all Revisited: A Semiautomatic Online Editor for Medieval Music Notations
Andreas Hartelt and Frank Puppe revisited their online editor for medieval music notations, enhancing usability and automation.

Enhancing Handwritten Music Sheet Datasets Using Generative Adversarial Networks (GANs)
K. R. Palavala and collaborators demonstrated how GANs could enrich handwritten music datasets for better OMR training.

Crafting Handwritten Notations: Towards Sheet Music Generation
N. Tirupati and team explored generative methods for creating handwritten sheet music, pushing the boundaries of synthetic data generation.



## The 5th International Workshop on Reading Music Systems 2023

The 5th International Workshop on Reading Music Systems (WoRMS) took place on November 4th, 2023, in Milan, Italy, offering both on-site and remote participation. This year’s workshop showcased a range of cutting-edge research in Optical Music Recognition (OMR), including advancements in medieval music manuscripts, handwritten music synthesis, and few-shot learning. The program featured three engaging paper sessions and a thought-provoking keynote by Werner Goebl, emphasizing community-driven solutions to perfect music score corpora.

Keynote: Perfecting Music Scores with Crowd-Sourced Validation
Werner Goebl from the University of Music and Performing Arts Vienna delivered the keynote titled "The Final Stretch of OMR: Perfecting Music Score Corpora with Browser-Based Editing and Validation." Goebl introduced mei-friend, a browser-based editing and validation interface that facilitates community-driven corrections to OMR-generated scores. This innovative tool juxtaposes source score images with their digital renderings, enabling crowdsourced validation to ensure pristine accuracy. The keynote sparked discussions on how decentralized, open systems can contribute to a FAIR (Findable, Accessible, Interoperable, and Reusable) digital music ecosystem.

Highlights from the Paper Sessions
Session 1: Historical and Multicultural Perspectives
Optical Music Recognition Workflow for Medieval Music Manuscripts
Ichiro Fujinaga and Gabriel Vigliensoni explored workflows tailored to the unique challenges of medieval manuscripts, emphasizing the importance of preserving historical notations.

The Suzipu Musical Annotation Tool
Tristan Repolusk and Eduardo Veas introduced Suzipu, a tool designed to create machine-readable datasets for ancient Chinese music, bridging gaps in cultural heritage digitization.

The OmniOMR Project
A comprehensive framework for OMR, presented by Jan Hajič, jr., and collaborators, addressing multi-lingual and multi-modal requirements for diverse music scores.

Towards Music Notation and Lyrics Alignment
Juan Carlos Martinez-Sevilla and Francisco J. Castellanos shared a case study on aligning notation and lyrics in Gregorian chants, advancing research in music-text alignment.

Session 2: Data Generation and Enhancement
Symbol Generation via Autoencoders for Handwritten Music Synthesis
Jonáš Havelka and team showcased how autoencoders can generate handwritten music symbols, contributing to realistic data synthesis.

Towards Artificially Generated Handwritten Sheet Music Datasets
Pranjali Hande and collaborators presented methods for generating artificial datasets, addressing the scarcity of annotated data in OMR.

Improving Sheet Music Recognition with Data Augmentation and Image Enhancement
Zihui Zhang and colleagues discussed innovative techniques for improving OMR accuracy, leveraging data augmentation and image preprocessing.

Session 3: Few-Shot and End-to-End Learning
Rotations Are All You Need: A Generic Method for End-To-End OMR
Antonio Ríos-Vila introduced a novel approach using rotations to simplify and improve end-to-end OMR pipelines.

Few-Shot Music Symbol Classification via Self-Supervised Learning
María Alfaro-Contreras presented a method for classifying music symbols with minimal training data, leveraging self-supervised learning.

Few-Shot Learning for Layout Analysis of Music Scores
Francisco J. Castellanos and team conducted a preliminary study on applying few-shot learning to the layout analysis of complex music scores.


## The 4th International Workshop on Reading Music Systems 2022

The fourth edition of the International Workshop on Reading Music Systems (WoRMS) was held last Friday, offering another dynamic hybrid experience. Researchers and industry professionals in Optical Music Recognition (OMR) came together to explore the latest advancements and challenges in the field. This year’s workshop featured nine diverse papers spanning topics such as dataset challenges, deep learning innovations, and integration of language models, alongside an engaging keynote by Marie Chupeau (Magic LEMP).

Keynote: Advancing OMR with MaestrIA
Marie Chupeau opened the workshop by presenting Magic LEMP's OMR solution, MaestrIA, developed for Newzik. She shared insights into their experiments with neural architectures for sequential, mask, and object detection approaches, highlighting the potential of score generation as an asset in OMR. Her discussion delved into the design process of their score generator, sparking interest and questions about how such systems might reshape music digitization.

Highlights from the Paper Sessions
Session 1: Datasets and Training Challenges
Challenging Sources: A New Dataset for OMR of Diverse 19th-Century Music Theory Examples
Fabian C. Moss and collaborators introduced a dataset targeting diverse 19th-century music theory materials. The dataset addresses the need for varied and challenging OMR training sets, opening avenues for more robust systems.

CompIdNet: Sheet Music Composer Identification Using Deep Neural Networks
Dnyanesh Walwadkar and colleagues presented their work on composer identification through deep learning. This research adds a novel dimension to OMR by linking sheet music to its creators.

Obstacles with Synthesizing Training Data for OMR
Jiří Mayer and Pavel Pecina explored challenges in generating synthetic training data, identifying issues that arise when bridging the gap between synthetic and real-world datasets.

Session 2: Full-Page and Graph-Based OMR
End-To-End Full-Page Optical Music Recognition of Monophonic Documents via Score Unfolding
Antonio Ríos-Vila and team proposed a full-page recognition pipeline leveraging a score unfolding mechanism for monophonic documents, addressing scalability in OMR systems.

End-to-End Graph Prediction for Optical Music Recognition
Carlos Garrido-Muñoz and collaborators discussed graph-based models, which allow for efficient encoding of complex music notation relationships.

Efficient Approaches for Notation Assembly in Optical Music Recognition
Carlos Penarrubia and co-authors presented techniques for efficiently assembling notations post-recognition, optimizing the final stages of the OMR pipeline.

Session 3: Applications and Enhancements
Computer-Assisted Measure Detection in a Music Score-Following Application
Eran Egozy and Ian Clester focused on integrating measure detection for applications in score-following, emphasizing real-time performance aids.

Automated Transcription of Electronic Drumkits
Florent Jacquemard and team showcased their work on transcribing electronic drumkit performances, bridging the gap between electronic and traditional score representation.

Improving Handwritten Music Recognition through Language Model Integration
Pau Torras and colleagues demonstrated how language models could enhance recognition accuracy, particularly for handwritten scores.



## The 3rd International Workshop on Reading Music Systems 2021

![worms2021]({% link /assets/img/worms2021.png %})

The third edition of The International Workshop on Reading Music Systems (WoRMS) was held in a hybrid live/virtual setting last Friday, 23rd of July. It brought together many researchers working in Optical Music Recognition (OMR) and also from the industry. This edition 11 papers researching a broad list of topics in OMR were presented, and an [outstanding keynote from Anthony Wilkes (Organum Ltd)](https://drive.google.com/file/d/1IDgOaW8tGxJt9Top7x7GSZXLsAo-XVcb/view) was talking on The design of ReadScoreLib. 

Below I will try to summarise some of the papers presented. 

[Hybrid Annotation Systems for Music Transcription by Ioannis Petros Samiotis, Christoph Lofi and Alessandro Bozzon](https://drive.google.com/file/d/17BdTUfU6Fk8qyrpxo6L-BGTqIhLvStL5/view)

Dwells on the idea of bringing human annotation and automated methods together for music transcription. In other words, how can a non-specialist carry out music transcription with careful task interaction using AI automated methods. Among 144 workers that executed tasks in MTurk, those with formal knowledge in music were rare. Audio extracts of target music scores were offered to increase performance, especially for short segments of one or two measures. For longer segments, audio extracts have shown better results against textual measures, but a combination of the two was used as more preferable. 


[Implementation and evaluation of a neural network for the recognition of handwritten melodies by Nils Wenzlitschke](https://drive.google.com/file/d/17Dp9gIjQPZVwSFJzKK8QA6Xjcgv894wj/view)

This research came as a fruit of a current need for digital archiving and digitalisation of music for the
University Library of Regensburg. It evaluates if the existing SOTA deep learning architecture is able to recognise handwritten monophonic scores for the purpose of digitalisation. Based on existing work, the architecture includes two neural networks: a stave recognition network using autoencoders and an end-to-end note recognition using recurrent convolutional networks. One limitation mentioned is the amount of annotated data available for this research. 


[DoReMi: First glance at a universal OMR dataset by Elona Shatri and George Fazekas ](https://scholar.google.com/scholar_url?url=https://arxiv.org/abs/2107.07786&hl=en&sa=T&oi=gsb&ct=res&cd=0&d=14810089764046134622&ei=zZr-YJCFNYqImgHdz7SYBQ&scisig=AAGBfm3Isc8XU8MWS1mRRgnn5ctiET7y8g)

We were also part of this workshop presenting our work in our newly published dataset DoReMi. We presented some of the challenges the lack of a well-annotated, that supports more than one stage of OMR poses and how DoReMi moves closer to such dataset. Furthermore, statistics on the dataset and baseline experiments on object detection using Faster R-CNN models. 

[The Challenge of Reconstructing Digits in Music Scores by Alexander Pacha](https://drive.google.com/file/d/10uUCaORERAzD-ISSm6FUeNNOOzrTRDzF/view)

Pacha presented some focused research he is currently conducting at e[note](https://enote.com/index) in recognising and reconstructing the digit elements in sheet music. He shows the main challenges posed by the ambiguity of the variations in their classes, their contextual nature and more computer vision issues. He then shows the results in using deep learning to recognise digits. The network was trained in synthetic samples and achieved a validation accuracy of 95%, which does not live it up in real-world scores. To address it was fine tunned on 7000 manually annotated real scores, but yet again, accuracy does not reach 60%. In the end, this opened up a long discussion in the workshop on why does this happen and the ways to tackle it. 

[Detecting Staves and Measures in Music Scores with Deep Learning](https://drive.google.com/file/d/1uSIrbiLrx1RfXEV86STS7XRuwJoa34O7/view)

This paper investigates strategies of detecting measures, staves and system measures using machine learning. That is to aid the detection of structural elements as a basis for an OMR system. A neural network is trained in handwritten music scores to generate annotations for typeset music. Detectron2 was used as a framework and Faster R-CNNs as a model to predict the bounding boxes in images. The datasets used for training were MUSCIMA++ and AudioLabs datasets. They applied the model in three settings: single class models (system measures, stave measures, staves), two class models (system measures & staves) and three class models (system measures & stave measures & staves). The first setting is performing best. However, considering that that model lacks diversity, it might not work well for every kind of sheet music. 

[Unsupervised Neural Document Analysis for Music Score Images](https://drive.google.com/file/d/1ZBRaOwsTkdOUo6sfm9xdQPuPMNyM89ho/view)

Given the lack of large training annotated set, this study suggests using Domain Adaptation (DA) based on adversarial training. The propose combining DA and Selectional Auto-Encoders for unsupervised document analysis. They utilise three corpora manually labelled for the layers: SALZINNES, EINSIEDELN and CAPITAN, and using F-score as an evaluation metric. Results obtained show the proposed method slightly improves state-of-the-art, but such adaptation should not be carried out in every type of layer. 

[Multimodal Audio and Image Music Transcription](https://drive.google.com/file/d/1ZDlU0WDmqC4-37s2gkCf2nOAt4Z5Ow-S/view)

This paper draws attention to Optical Music Recognition (OMR) and Automatic Music Transcription (AMT) similarities and exploits them to assist each field. The paper presents a proof-of-concept that combines end-to-end AMT and OMR systems predictions over a set of monophonic scores. Using Symbol Error Rate (SER), they show that a fusion model of the two can slightly improve the error rate in OMR.

[Sequential Next-Symbol Prediction for Optical Music Recognition](https://drive.google.com/file/d/1o4zm_fx_Fa7zclWkqgbVLx2x3DuvZidz/view)

This study proposes to address the lack of large training sets with a sequential classification-based approach for music scores. That is by predicting the symbol locations and their respective music-notation label using Convolutional Neural Networks (CNN). 

[Completing Optical Music Recognition with Agnostic Transcription and Machine](https://drive.google.com/file/d/1WAhrcPRzpuoB1fJsMkGCZamIp1CHv3c5/view)

This work focuses on the last stage of OMR, encoding, where outputs from images are converted to a score encoding format. The paper investigates the implementations of recognition pipelines that use Machine Translation to do the encoding. 



## Centre for Doctoral Training in AI and Music (AIM) & Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT) virtual visit

Remote working has limited us in many things, simultaneously it gave us the liberty of being in places without taking an aeroplane. Virtual visits became a much easier thing to organise. That allowed us to meet our peers in IRCAM’s Artificial Creative Intelligence and Data Science (ACIDS) team in Paris back in February.  This time we are delighted to “travel” to Canada and meet with Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT).

We will share our ideas and research and what our groups are working on at the moment. Each centre will have 6-7 presentations from PhD students, totalling 12-14 presentations, with a length of 3-5 minutes per presentation.

These presentations will revolve around topics such as deep learning, sound synthesis, gesture and performance analysis, artificial creativity, augmented instruments, generative music, optical music recognition, composition… and much more! See the agenda!

Date: 30th of June 2021

Time: 2-5 pm (UK time, GMT+1)

Where: https://mcgill.zoom.us/j/83537509055


Organisers:

AIM
Elona Shatri: e.shatri@qmul.ac.uk
Nick Bryan-Kinns: n.bryan-kinns@qmul.ac.uk

CIRMMT
Eduardo Meneses
Carolina Rodríguez

## Optical Music Recognition: State of the Art and Major Challenges
### Review paper summary on OMR — paradigm shift and possible directions.

Recently I got my very first paper accepted to the [International Conference on Technologies for Music Notation and Representation (TENOR) 2020](https://arxiv.org/abs/2006.07885). The journey of getting published was very insightful and it will serve as my own guide to publishing in the future.

The paper summarises prior work and takes a position in progressing the field of my research topic — Optical Music Recognition (OMR). You can read more about OMR in my previous article. I have heard the pros and cons of publishing a position paper at the beginning of my academic journey. However, writing this paper made me doubt myself which always resulted in learning more.

Back to the actual content of the paper, I try to summarize the four main stages of the OMR pipeline variety of published work in each stage. Furthermore, I try to capture the paradigm shift in the methods used in OMR from conventional computer vision systems to end-to-end deep learning networks.


Overall OMR traditional pipeline [13]
Initially, the four stages of OMR included image preprocessing, musical object detection, musical symbol reconstruction and finally encoding the musical knowledge into a machine-readable file. In the image preprocessing stage mainly enhancement, de-skewing, blurring, noise removal and binarisation were applied [1, 2, 3, 4, 5]. Binarisation is the process of converting an image to binary (only black and white pixels). Initially, such processes were performed using traditional techniques such as choosing a binarisation threshold based on the global histogram of the image. Later on, for instance, binarisation is done using sectional auto-encoders [6, 7]. These encoders learn an end-to-end transformation for the binarisation.

Moving on to musical symbol detection, this stage has three substages: staff-processing, musical symbol processing and finally classification. In staff-processing, staff lines are first detected and depending on the study removed. Lately, Pacha et al. using object detection techniques proved that removing staff lines does not guarantee better performance [8].

The musical object detection stage has largely benefited from the state of the art in computer vision, especially from object detection in general. Models such as Fast R-CNNs, Faster R-CNNs, Single Shot Detectors (SSD) were used to detect musical objects. They use pre-trained models which are later fine-tuned in a handwritten sheet music dataset MUSCIMA++ [9]. This work draws a baseline on using deep learning in object detection in sheet music.

One of the most complicated stages is reconstructing structural and semantic relationships between the musical symbols. This step was usually done using musical knowledge, rules and heuristics [10, 12]. Recently, this stage was also exposed to deep learning methods and end-to-end learning [11]. However, a major problem here is finding representations that can capture both structural and semantic relationships in music. This is due to the fact that music has a very complex structure with the symbols having spatial relationships and long-term dependencies. These relationships build up the music a structure and their semantic meaning is the music itself. As such, finding a representation that embeds all this information is very challenging.

Ultimately, the goal is to encode all retrieved relationships into a machine-readable file. There is a variety of such formats. While some formats encode instrument, pitch, velocity and onsets, those can only facilitate replayability. Other formats can encode more information which facilitates not only replayability but also an approximation on how the symbols looked on the sheet.


OMR moving towards end-to-end learning [13]
To conclude the main challenges on OMR today are the lack of a bigger labelled dataset, music objects and staff lines detection, semantic meaning reconstruction, and lack of standardisation, evaluation metrics and the output representation [13].

Read more here: https://arxiv.org/abs/2006.07885

References

I. Fujinaga, “Optical music recognition using projections,” PhD dissertation, McGill University Montreal, Canada, 1988.
B. Couasnon, P. Brisset, I. Stephan, and C. P. Brisset, “Using logic programming languages for optical music recognition,” in In Proceedings of the Third International Conference on The Practical Application of Prolog. Citeseer, 1995.
A. Fornes, J. Llados, G. Sanchez, and H. Bunke, “Writer identification in old handwritten music scores,” in 2008 The Eighth IAPR International Workshop on Document Analysis Systems. IEEE, 2008, pp. 347– 353.
A. Fornes, J. Llados, G. Sanchez, and H. Bunke, “On the Use of Textural Features for Writer Identification in Old Handwritten Music Scores,” in 2009 10th International Conference on Document Analysis and Recognition. Barcelona, Spain: IEEE, 2009, pp. 996–1000. [Online]. Available: http://ieeexplore.ieee. org/document/5277541/
L. J. Tardon, S. Sammartino, I. Barbancho, V. Gomez, and A. Oliver, “Optical music recognition for scores written in white mensural notation,” EURASIP Journal on Image and Video Processing, vol. 2009, no. 1, p. 843401, 2009.
A.-J. Gallego and J. Calvo-Zaragoza, “Staff-line removal with selectional auto-encoders,” Expert Systems with Applications, vol. 89, pp. 138–148, 2017.
J. Calvo-Zaragoza and A.-J. Gallego, “A selectional auto-encoder approach for document image binarization,” Pattern Recognition, vol. 86, pp. 37–47, 2018.
A. Pacha, K.-Y. Choi, B. Couasnon, Y. Ricquebourg, R. Zanibbi, and H. Eidenberger, “Handwritten Music Object Detection: Open Issues and Baseline Results,” in 2018 13th IAPR International Workshop on Document Analysis Systems (DAS). Vienna: IEEE, Apr. 2018, pp. 163–168. [Online]. Available: https: //ieeexplore.ieee.org/document/8395189/
Hajič, Jan, and Pavel Pecina. “The MUSCIMA++ dataset for handwritten optical music recognition.” 2017 14th IAPR International Conference on Document Analysis
P. D, “Computer pattern recognition of standard engraved music notation,” in Structured Document Image Analysis. Springer, 1992, pp. 405–434.
A. Pacha, J. Calvo-Zaragoza, and J. Hajic jr, “Learning notation graph construction for full-pipeline optical music recognition,” in 20th International Society for Music Information Retrieval Conference, 2019.
D. Bainbridge and T. C. Bell, “A music notation construction engine for optical music recognition,” Softw., Pract. Exper., vol. 33, pp. 173–200, 2003.
E. Shatri and G. Fazekas, “Optical Music Recognition: State of the Art and Major Challenges”, arXiv preprint arXiv:2006.07885, 2020

## A review on Generative Adversarial Networks
### How did the GANs change the way machine learning works?

The history of deep learning has shown to be a bit unusual. Many practices, such as convolutional neural networks, invented in the 80s, had a comeback only after 20 years. While most of the methods had a comeback, Generative Adversarial Networks were one of the most innovative techniques to happen to deep learning in the past ten years. While discriminative networks with propagation and dropout algorithms with a well-behaved gradient shown to be very successful, it was not the same case with generative networks. Deep generative networks had issues with approximating intractable probabilistic computations during the estimation of maximum likelihood. Furthermore, it can not leverage the benefits of linear units in a generative context. GANs came to assist the field with these two issues while bringing both a generative and a discriminative network together.

GANs were first proposed by Goodfellow et al. [1] at the University of Montreal. The basic framework contains a generator working against an adversary, while the discriminator learns to tell if a sample belongs the data distribution or from the generative network. The idea is for these two networks to get better while competing against each other.

The most straightforward modelling is having both the discriminator and generator as a multilayer neural network. The generator learns the mapping from a latent space to data distribution, tending to become similar to the ground data distribution. The discriminator, on the other hand, tries distinguishing between real data distribution and what was generated from the generator. The goal of the generative network is to trick the discriminator into thinking that the novel data produced is coming from true data distribution; this way, it increases the discriminator’s error rate.

Deep Convolutional Generative Adversarial Networks(DCGANs) – mc.ai
Figure 1: Building blocks of a GAN (https://mc.ai/deep-convolutional-generative-adversarial-networksdcgans/)
We should emphasise that the role of GAN is not to reproduce data used during training, instead to produce new data. We can describe it as a two-person game, these two networks opposing each-other, meaning that the end goal is achieving an equilibrium in which these trained networks have the best response to each other. At this point, they can not improve anymore, and the training stops. However, such an equilibrium is difficult to be achieved and even less maintained, and this is the first issue with GANs. Another problem is that there is no way to validate if the generator has learned to produce a distribution similar to real-life data distribution in a held-out dataset like other deep learning techniques.

In the original paper, it is experimentally shown that the amount of data and the depth of the network plays a huge role in a better performance. When the data point is an image that would mean that the amount of data should be exponential to the number of pixels. Given that images have hundreds/thousands of pixels that would mean better results are achieved in nets that can not be implemented yet with the available computational power and data.

GANs application has been extensive, from art, fashion, advertising, science to video games. However, these networks have also been adopted for malicious intents such as creating fake social media profiles using synthesised images produced with GANs. As we can see, its application is more extensive in the field of computer vision.


Figure 2: A road map of GANs since the original paper, inspired by [9]
In Figure 1 a road map of GANs starting from the original paper is given. Because of the page limitation, I will list the mentioned methods briefly and what they tackle — many papers proceeding original work focus in modification during the training process.

Deep convolutional GANs (DCGANs) [2] have better performance since instead of defining the generator (G) and discriminator (D) with multilayer perceptrons it defines it with CNNs, when used with images. It does not have pooling layers, so to increase spatial dimensionality, it uses deconvolution. Normalise batches for all layers in G and D except for the last layer of D and the first layer of G, so the information on the correct mean of data distribution is not lost.

Changes in the training settings were proposed by ImprovedGANs [3] that have to do with minibatch discrimination, virtual batch normalisation and feature matching. Given that the original GANs suffer from low-resolution, LAPGAN [4] using CNNs within a Laplacian pyramid generate higher resolution images. Progressive GANs (PGGAN) [5] also propose a modification in training, based on progressive neural networks, to grow both discriminator and generator, from low to higher resolution by adding new layers progressively.

image-to-image translation traditionally to learn to map between an output and input image using a training set that contains aligned pairs. CycleGANs [6] use an adversarial loss to map an image from source domain X to a target domain Y, lacking pairs. Furthermore, they couple this loss with an inverse mapping achieving a cycle constancy.

Another issue with original GANs is mode collapse, which means they tend to produce similar samples, even when trained on diverse datasets. PACGANs [7] handle this issue with what they call packing. The main changes happen in the discriminative network, enabling the network to make decisions based on multiple samples from the same class, from both real and generated data distribution.

Self-Attention Generative Adversarial Network (SAGAN) [8] propose using long-range dependency modelling with attention for image generation. It uses spectral normalisation for G and D, and prove to improve the training process.

Another different way of using GANs has been training the generator on a single natural image, using a pyramid of FC GANs, each learns a distribution at different scales of the image.

A problem yet to be tackled in GANs is the fact that they assume that the generated samples have different generative parameters, which means they can not produce discrete data directly. Another open question is how to measure the uncertainty of a well trained generative network.

I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, “Generative adversarial nets,” in advances in neural information processing systems, pp. 2672–2680, 2014.
A. Radford, L. Metz, and S. Chintala, “Unsupervised representation learning with deep convolutional generative adversarial networks,”arXiv preprint arXiv:1511.06434, 2015.
T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen, “Improved techniques for training gans,” in Advances in neural information processing systems, pp. 2234–2242, 2016.
E. L. Denton, S. Chintala, R. Fergus, et al., “Deep generative image models using a laplacian pyramid of adversarial networks,” in advances in neural information processing systems, pp. 1486–1494, 2015.
T. Karras, T. Aila, S. Laine, and J. Lehtinen, “Progressive growing of gans for improved quality, stability, and variation,”arXiv preprint arXiv:1710.10196, 2017.
J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, “Unpaired image-to-image translation using cycle-consistent adversarial networks,” in Proceedings of the IEEE international conference on computer vision, pp. 2223–2232, 2017.
Z. Lin, A. Khetan, G. Fanti, and S. Oh, “Pacgan: The power of two samples in generative adversarial networks,” in Advances in neural information processing systems, pp. 1498–1507,2018.
H. Zhang, I. Goodfellow, D. Metaxas, and A. Odena, “Self-attention generative adversarial networks,”arXiv preprint arXiv:1805.08318, 2018.
J. Gui, Z. Sun, Y. Wen, D. Tao, and J. Ye, “A review on generative adversarial networks: Algorithms, theory, and applications,”arXiv preprint arXiv:2001.06937, 2020.


## Online VISUM 2020 highlights 

Starting from 3rd to July 10th, I attended the VISUM summer school online. The conference was set to happen in Porto, then the pandemic and it was moved online.

The itinerary included lectures, AI shots, and the main competition. The lectures were in ranges of machine learning, deep learning, information security, AI and more. While the AI shots were the sessions between lectures and breaks where people from industry partners were invited to give a 30 minutes talk on their work and what are the AI use cases in their work.

Jaime Cardoso (INESC TEC and the University of Porto FEUP), gave a detailed lecture on computer vision (CV) and machine learning (DL). A roadmap and derivatives such as deep learning, artificial intelligence and their history.

One of the highlights of the summer school was having professor Johan Suykens as an invited speaker to talk on Deep Learning and Kernel Machines. As a reminder, least-squares SVM (LS-SVM) classifiers were proposed by professor Suykens and Vandewalle [1]. After giving a brief introduction to deep learning, he went on to talk more on function estimation and model representation, LS-SVM, kernel spectral clustering, restricted kernel machines and generative models.

Action recognition is one of the most popular and explored areas in computer vision. We had the pleasure to listen to Pascal Mettes from the University of Amsterdam give a detailed talk on action recognition. His talk had three main topics: video representation for action recognition, action and video understanding without labels and recent advances in action understanding. On recognizing video with deep learning, he outlined three approaches: pooling over time, time as a channel dimension and time as a spatial dimension. Furthermore emphasizing the pros and cons of using one approach over the others. Moving to the second part, he mentions approaches such as zero-shot learning, self-supervised learning for action recognition. He closed his talk with recent approaches to action recognition using graph neural networks.

A very interesting talk was Optimal Transport in Deep Learning, given by professor Nicolas Courty. He started his talk with an introduction to optimal transport and then moved to its application to DL. The lecture went back to the first formulation of the optimal transport and resource allocation problem by Monge. He then moved to other contributions to the field such as Tolstoi, Kantorovich, Hitchcock. He explained learning from distribution, histograms, Wasserstein Loss, finding the Monge mapping and the domain adaptation problem. What was clear from the lecture is that optimal transport is a well-grounded theory allowing us manipulating with distributions in ML. It is a very complex theory; nonetheless, it is widely applied in large scale deep learning settings.

One of the talks I was very familiar with was the talk on Information Security by professor Marta Gomez-Barrero. She introduced the whole field going on to cryptography, biometric data, user authentication and discussing a case study. The case study was Cancelable Biometrics Based on Bloom Filter [2].

Last but not least, a lecture on Interpretability for ML in Medical Imaging from Mauricio Reyes (University of Bern). Terms like explainability and interpretability are often used interchangeably in ML. Reyes explains the difference between these two :
Explainability — “What’s the process behind” (e.g. Apple falling from a tree: gravity)
Interpretability — “understanding/predicting causal/effect phenomena” (what happens if I cut the apple from the tree: it will fall)
He goes on to explain why do we interpretability of models and especially in medical imaging. He later uses popular ML models with use cases and how can they be interpreted.

The most exciting part of the summer school was the competition. From 70+ participants, we formed teams of three to work on the same problem. The problem we worked on was fish detection. The winning teams had similar results to state of the art. Similar techniques in image pre-processing and methodologies were used, such as YOLO, Faster R-CNN and the real boost coming from transfer learning using pre-trained models.

[1] Suykens, J. A. K., et al. “Least squares support vector machine classifiers: a large scale algorithm.” Proceedings of the European Conference on Circuit Theory and Design. Vol. 10. 1999.

[2] Rathgeb, Christian, et al. “Towards cancelable multi-biometrics based on bloom filters: a case study on feature level fusion of face and iris.” 3rd international workshop on biometrics and forensics (IWBF 2015). IEEE, 2015.

## What is Optical Music Recognition?

I have always been enthusiastic to explore of Computer Vision, equally, music has also been a great part of my life. Combined, they make a great research problem! Four months ago, I commenced a 4-year Ph.D. program. So far, it has been an insane journey in terms of how much I have learned and the enthusiasm to learn more.


Photo by Sarah on Unsplash
The research problem I am undertaking is Optical Music Recognition (OMR), more pointedly, investigating if Deep Learning can assist in improving the performances of the current methods.

For you to get to comprehend this problem a little bit more, I will attempt to clarify what OMR is, the conventional methods used and the main issues needed to be tackled in the future.

Most of us have presumably used Google translate and its camera translation feature by now. By just taking a picture of a text, we save time and avoid learning Chinese or other languages. Now let us think of how this feature would apply to music. Musicians still write in music sheets or blank paper. However, if they want to share their music, they will have to transcribe it into a computer. A computer-readable music file would be more accessible. Therefore the motivation behind this research is the possibility of allowing composers, musicians, to not only transcribe and edit music by means of taking a picture of the sheet music but ultimately share and play their pieces. OMR would also assist in music statistics, and enable searchability for notations, similar to that of searching for text.

Calvo-Zaragoza et al. give a very clear and inclusive definition of OMR, calling it a research field rather than a simple problem.

Optical Music Recognition is a field of research that investigates how to computationally read music notation in documents.

The second part of the definition stresses the “computationally read music notation in documents”, given that it is performed by computers (rather than humans), it does not concern music notation models themselves, but it builds upon this knowledge. Furthermore, it emphasizes the information captured by these systems, which I will explain in more detail in below sections.

The research field was established at MIT in the late 1960s, using scanned printed music sheets. The pioneers in the field are Ichiro Fujinaga, Nicholas Carter, Kia Ng, David Bainbridge and Tim Bell. Their work is still an excellent foundation for today’s research. OMR is related to other fields such as music information retrieval, computer vision and document analysis.

Based on the carried out studies a standard pipeline reflecting the approaches taken into solving the problem was formed (see figure 1).


Figure 1. The standard OMR pipeline (Image by author)
The usual inputs to this pipeline are scans/pictures of printed/handwritten music sheets. These images are then subject to image processing techniques. These techniques include binarization (black and white), blurring, deskewing (rotation), and will help in reducing the noise in the image.

Enhanced images will next be used in music object recognition. In this step, the algorithm will try to identify musical objects such as clefs, noteheads, bars, slurs, and others. In this stage, the objects are primitives and separate from their semantic meaning.


Figure 2. Object detection with Faster R-CCN (source: https://github.com/apacha/MusicObjectDetector-TF)
Consequently, the next step attempts to reconstruct the relationships these primitives have had, together with the semantic meaning. This approach rebuilds the semantics based on grammar rules that exist in music.

The final output can represent the musical meaning and description of the music score in the input and it is machine-readable. The usual formats of these files can be MIDI, MusicXML, MEI and so on.


Sample of a file format output (Image by author)
We want to explore new ways of performing such steps using Deep Learning (DL). Most DL models build on artificial neural networks. These networks are inspired by the biological neural networks. They consist of many layers that have the so-called nodes; they contain one input layer, one or more hidden layers and output layers. The deeper it goes, the more intricate features a model can learn and extract. The hidden layers in between are usually referred to as a “black box.” That is because we can not easily understand what happens inside, albeit new research is focusing on this.

We plan to start by applying this approach in the second stage of OMR, that is, music object detection. To do this, we need a vast dataset containing images of music sheets. This dataset should also have a ground truth so that the model can learn well from it. A part of the data, called test data, should not be seen by the model. This way, we can evaluate how good the model does on things it has never seen before. This model should be designed based on the nature of the experiment, the input, and the desired output. We also propose bringing standardization on what formats inputs, outputs should be and their evaluation.

References:

A. Rebelo, I. Fujinaga, F. Paszkiewicz, A. R. S. Marcal, C. Guedes, and J. S. Cardoso, “Optical music recognition: state-of-the-art and open issues,” Int J Multimed Info Retr, vol. 1, no. 3, pp. 173–190, Oct. 2012. [Online]. Available: http: //link.springer.com/10.1007/s13735–012–0004–6

J. Calvo-Zaragoza, J. Hajicˇ Jr., and A. Pacha, “Understanding Optical Music Recognition,” arXiv:1908.03608 [cs, eess], Aug. 2019, arXiv: 1908.03608. [Online]. Available: http://arxiv.org/abs/ 1908.03608

Pacha, Alexander, Jan Hajič, and Jorge Calvo-Zaragoza. “A baseline for general music object detection with deep learning.” Applied Sciences 8.9 (2018): 1488.





## ICASP 2020 - A review on my favourite papers

### On Network Science and Mutual Information for Explaining Deep Neural Networks

This paper works toward interpretable neural network models. This work is in part of a bigger move in the machine learning community, to open the so-called “black box” and be able to explain how the machine is learning. This study investigates how the information flows through feedforward networks. They propose using information theory on top of the network science to calculate an information measure that represents the amount of information that flows between two neurons. The technique to codify this information flow is called Neural Information Flow (NIF). Basically, NIF weights the importance that edges of the neurons have in a multilayer perceptron (MLP) or Convolutional Neural Networks (CNN) while using the mutual information between nodes which is modelled as distribution. Feature attribution is computed as follows, an importance value is placed along all the edges of the network, a product of all these values in a given path is calculated, to finally sum all these products across all possible paths from an input and output. NIF provides information on the most crucial paths of a network. Hence, less important parameters can be removed without loss of accuracy, facilitating network pruning at inference time. Furthermore, NIF can help in visualising edge communities, understanding how nodes form communities, for instance in an MLP. This could help in better training of a network, but needs to further be investigated. However, NIF is of a high computation complexity, which seems to be the main area for improvement.

[Davis, Brian, et al. “On Network Science and Mutual Information for Explaining Deep Neural Networks.” ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020](https://ieeexplore.ieee.org/document/9053078)

 

### Towards High-Performance Object Detection: Task-Specific Design Considering Classification and Localization Separation

This paper tackles the efficiency of object detection. Object detection is a process of simultaneous localisation and classification. While the first one gives the category the object belongs to, the second one tells where this object is located. Both tasks require robust features that well represent an object. However, these tasks have many non-shared characteristics. Classification concentrates on partial areas or the most prominent region during recognition, i.e. the head of a cat, whereas localisation considers a larger area of the image. Classification is translation invariant while localisation has translation variant characteristics. Hence, the authors propose a network that in addition to considering the common properties, also considers task-specific characteristics of both tasks. They propose altering existing object detection in three stages. Having a lower layer that shares less semantic features between classification and localisation. Consequently, separating the backbone layers to learn task-specific semantic features. Finally, fuse these two separated features by concatenating and 1×1 convolution to have the same number of channels with the separated features. Experimental results show that such an approach can encode two-task specific features while improving performance. However, these improvements are not substantial and further detailed investigation is needed for the task-specific objective functions. 

[Kim, Jung Uk, et al. “Towards High-Performance Object Detection: Task-Specific Design Considering Classification and Localization Separation.” ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020](https://ieeexplore.ieee.org/abstract/document/9054600)



### Unsupervised Domain Adaptation for Semantic Segmentation with Symmetric Adaptation Consistency

Domain adaptation deals with learning a predictor when the training and test sets come from a different distribution. An example of this situation could be semantic segmentation. If a network trained in synthetic images, fully labelled, has to segment real-world images. These two types of distributions are very different; therefore, a mapping of features is needed. Unsupervised domain adaptation uses the labels from the training time to solve tasks in the shifted distribution data with no labels. This paper utilizes adversarial learning and semi-supervised learning for domain adaptation in semantic segmentation. The two stages of this method are image-to-image translation and feature-level domain adaptation. Firstly, images from source domain are translated to the targeted domain using a translation model. Finally, the semantic segmentation model is trained in an adversarial and semi-supervised manner at the same time. This is done by first symmetrically training two segmentation models with adversarial learning and then between the outputs of the two models introduce the consistency into semi-supervised learning to improve accuracy on pseudo labels that highly affect the final adaptation performance. They achieve state-of-the-art performance on semantic segmentation on the GTA-to-Cityscapes. 



[Li, Zongyao, et al. “Unsupervised Domain Adaptation for Semantic Segmentation with Symmetric Adaptation Consistency.” ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020](https://ieeexplore.ieee.org/document/9053388)


# Articles on Machine learning in Albanian language 


## Çfarë janë Rrjetat Nervore Kompjuterike (Neural Networks)?

Përmbajtja:
Përkufizimi i rrjetave nervore kompjuterike
Shembuj
Elementet e rrjetave nervore kompjuterike
Konceptet baze të rrjetave nervore të thella
Pëkufizimi i rrjetave nervore kompjuterike
Rrjetat nervore kompjuterike (Neural Networks) janë një bashkësi e algoritmeve përllogaritës, të modeluar sipas trurit të njeriut, e të projektuar që të njohin motive apo rregullsi. Çfarëdo qofshin të dhënat në hyrje: fotografi, tekst, seri kohore, këto rrjeta së pari i përkthejnë këto të dhëna në të dhëna numerike, që më pas vendosen nëpër vektorë, e në fund gjendet motivi.

Rrjetat nervore kompjuterike na ndihmojnë duke klasifikuar dhe grumbulluar (classifying and clustering). Këtë mund ta imagjinoni si një shtresë klasifikimi e grumbullimi në krye të të dhënave që posedoni. Këto janë të afta të njohin e identifikojnë të dhëna që nuk i ka parë më herët, pa etiketë, në baze të ngjashmërive që ka me të dhenat e tjera që i ka parë më herët. Për shembull: të imagjinojmë që gjatë jetës sonë kemi parë mollë të gjelbërta dhe të kuqe. Sot dikush na ofron një mollë të verdhë dhe ne shohim ngjyren, por ngjyra nuk na ndihmon ta identifikojmë cili frut është, shohim formën dhe tani e krahasojmë me të gjitha frutat që kemi parë më herët. Gjejmë frutin më të ngjashëm në formë, ose edhe në shije e ky frut do të ishte molla. Pra ne edhe pse kurre nuk kemi pare mollë të verdhë, prap jemi në gjendje të tregojmë se është mollë. I njejti parim vlen edhe për këto rrjeta. Në këtë rast, dy atributet e përmendura të pemës janë ngjyra dhe forma.

Deep learning (shqip: të mësuarit e thellë) përdor një bashkësi të dhënash të etiketuara për t’u trajnuar apo për të mësuar. Këtë mësim apo trajnim do t’a përdorë më vonë për të krahasuar me të dhëna të paetiketuara.

Tani të mendojmë për çfarë problemesh mund të përdoren këto rrjeta, dhe a mund të aplikohen në problemet që ju mund të keni. Për të ditur përgjigjen, duhen bërë më shumë pyetje, për shembull:

· Për çfarë rezultate jemi të interesuar? Rëndesi e madhe duhet t’i kushtohet etiketave. Shembull kemi, emailet që marrim çdo ditë e që shpesh janë spam. Në këtë rast etiketat mund të jenë spam dhe jo_spam, dhe kjo do të ndihmonte në filtrimin e emaileve. Për dikë që ka nje biznes dhe interesohet në atë se sa janë të lumtur klientët, etiketat mund të jenë klient_i_zemruar apo klient_i_kënaqur apo edhe etiketa tjera më neutrale.

· Pastaj duhet menduar a kemi të dhëna të mjaftushme për të mbështetur këto etiketa. A mund të gjejmë të dhëna të etiketuara, apo a mund të etiketojmë këto të dhëna vetë.

Disa Shembuj
Deep learning harton të dhënat që i merr në hyrje deri në rezultatet, duke gjetur korrelacione. Njihet ndryshe edhe si “universal approximator” (përafrues universal), kjo sepse mund të mësojë të përafrojë një funksion të panjohur f(x) = y, midis një hyrje x dhe një dalje y duke supozuar që të dyja kanë njëfarë korrelacioni. Gjatë procesit të të mësuarit, rrjeti nervor mund ta përshtati këtë funksion, duke gjetur funksionin më të përshtatshem për transformimin më të mirë të mundshëm të x në y. Pra ky funksion lehtë mund të ndryshojë nga f(x) = 2x +3 në f(x)= 6x+0.2.

Klasifikimi
Në një artikull të mëhershëm kam diskutuar më shumë mbi Supervised dhe Unsupervised Learning (të mësuarit me mbikqyrje dhe pa mbikqyrje).
Të gjitha operacionet e klasifikimit janë plotësisht të varura nga të dhënat e etiketuara. Kjo do të thotë se njeriu është burimi i parë dhe i vetëm i këtyre të dhënave të etiketuara; njeriu me njohurinë e tij siguron të dhena të etiketuara për këtë model. Kjo quhet ndryshe Supervised Learning (të mesuarit e mbikeqyrur). Klasifikimi mund të përdoret në:

Detektimin e fytyrave, njohjen e njerëzve në imazhe (p.sh. shpesh Facebook na kërkon të verifikojmë nëse fytyrat në foto janë të personave përkates), pastaj, njohjen e emocioneve nga shprehjet e fytyrës
Mund të identifikojë objekte në fotografi (shenja të komunikimit, këmbesorë në lëvizje etj.)
Detekton zëra, identifikon folësit, mund të përkthejë nga zëri në tekst, mund të njohë emocione në zë
Mund të dallojë apo klasifikojë emaile që janë spam (email mashtrues), mund të njohë emocione në tekst (p.sh. reagimet e klientëve)
Grumbullimi
Grumbullimi, apo në Anglisht “Clustering”, është detektimi i ngjashmërive mes të dhënave që posedojmë. Kjo metode nuk kërkon që të dhënat të kenë etiketa, prandaj dhe quhet Unsupervised Learning (të mesuarit pa mbikqyrje). Të dhëna e paetiketuara njanë shumica e të dhënave në botë. Parimi i “machine learning” është: sa më shumë të dhëna që algoritmi përdor gjatë fazës së trajnimit, aq më i saktë do të jetë. Prandaj grumbullimi ka potencial për të prodhuar një algoritëm më të saktë. Rastet ku grumbullimi përdoret më shumë janë:

Kërkimi: krahasimi i dokumentave, fotografive, zërave apo të ngjashme
Detektimi i anomalive: detektimi i sjelljeve të pazakonshme
Elementet e rrjetave nervore kompjuterike
Çdo rrjetë nervore kompjuterike është e përbërë prej disa shtresave. Këto shtresa janë të përbëra prej nyjeve (nodes). Nyjet janë pikërisht pikat ku zhvillohet përpunimi apo përllogaritja, që deri diku imitojnë neuronet në trurin e njeriut të cilat “ndizen” me stimulim të mjaftueshëm. Ato kombinojnë një bashkësi koeficientësh, peshash të cilat mund ta amplifikojnë apo dobësojnë hyrjen (inputin), duke përcaktuar kështu rëndësinë e hyrjeve në lidhje me detyrën që algoritmi përpiqet të mësojë; për shembull, cila hyrje është më e rëndësishme në klasifikimin e të dhënave me gabim më të vogël. Prodhimet e peshave me hyrjet mblidhen e pastaj shuma kalon nëpër të ashtuquajturat funksione aktivizimi (activation function) të nyjeve, për të përcaktuar nëse ky sinjal duhet të kalojë më tutje nëpër rrjet, në mënyrë që të ndikojë te dalja përfundimtare, për shembull gjatë klasifikimit. Nëse këto sinjale arrijnë të kalojnë nëpër këto nyje, mund të themi se neuroni është aktivizuar. Shikoni fotografinë e mëposhtme se si funksionon ky model.


Procesi i vendosjes se peshave dhe funksioni aktivizimit
Me x janë paraqitur hyrjet, me w peshat e hyrjeve që më tej kalojnë nëpër një shumëzues për të arritur tek funksioni aktivizimit, që në fund të gjenerojë një dalje.

Një shtresë neuronesh është një rresht me ndërprerës që punojnë si neurone që ndizen e fiken për aq kohë sa rrjeta ushqehet me hyrje. Çdo dalje nga një shtresë është në të njëjtën kohë hyrje për shtresën tjetër, duke filluar nga shtresa fillestare ku rrjeti ushqehet me të dhëna.

Për të përcaktuar domethënien apo rëndësinë që atributet e të dhënave në hyrje kanë, duhet çiftëzuar peshat e modelit me atributet në fjalë.

Konceptet bazë të rrjetave nervore kompjuterike
Rrjetet e të mesuarit të thellë (deep learning) dallohen nga rrjetet njështresore të fshehura nga thellësia e tyre; kjo thellësi përfaqëson numrin e shtresave me nyje nëpër të cilat të dhenat duhet të kalojnë deri sa rrjeti të arrijë të njohi një motiv.

Versione më të hershme të këtyre rrjetave kanë qenë më të cekta, duke përmbajtur vetëm një shtresë në hyrje, një në dalje, dhe vetëm një në mes. Prandaj çdo rrjetë që posedon më shumë se tri shtresa (duke numëruar hyrjen e daljen) kualifikohet si “e thelle” (“deep”).

Në këto rrjete, çdo shtresë e nyjeve trajnohet në atribute te ndryshme nga ajo se në çfarë është trajnuar shtresa para saj. Sa më shumë shtresa kalojnë, atributet që nyjet njohin janë më komplekse, kjo pasi që arrijnë të agregojnë dhe kombinojnë atribute nga shtresat e mëparshme. Ky proces njihet ndryshe si hierarki e atributeve, dhe eshte hierarki e rritjes së kompleksitetit edhe abstraksionit. Kjo i mundëson këtyre rrjetave të përballojnë një sasi shumë të madhe të të dhënave, me dimensione të larta me miliarda parametra që kalojnë nëpër funksione jolineare.

Mbi të gjitha, këto rrjete janë të afta të zbulojnë struktura të fshehura edhe në të dhëna të paetiketuara e pa strukturë, e që janë shumica e të dhënave në botë. Një emër tjetër për këto lloje të të dhënave është burime të paperpunuara (raw data), për shembull: fotografitë, tekstet, videot dhe audio inçizimet. Prandaj, një nga problemet që “deep learning” mundohet të zgjidh është gjetja e një metode më të mirë për përpunim dhe grumbullim të të dhënave të papërpunuara, e të paetiketuara.

Për shembull, “deep learning” mund të marrë një milion imazhe dhe t’i grumbullojë sipas ngjashmërive nëpër grupe. Si shembull keni telefonin tuaj të mençur që grumbullon fotografitë sipas personave apo kafshëve në foto. I njëjti parim vlen edhe me tekstet. Tekstet që kanë emocione apo fjalë të ngjashme do të grumbullohen nëpër grupe të veçanta. Kur të dhënat në hyrje jane seri kohore, të dhënat mund të mblidhen në dy grupe: normale (të shëndetshme), ose anomali (të rrezikshme). Nëse këto seri kohore gjenerohen nga një telefon i mençur, do të jepen më shumë njohuri mbi shëndetin e zakonet e përdoruesit; ka raste kur kjo ndihmon në parandalimin e katastrofave.

Rrjetat “Deep Learning” janë të afta të ekstraktojnë atribute në mënyrë automatike, pa ndërhyrjen e njeriut, ndryshe nga disa metoda më tradicionale në “machine learning” të cilat kanë algoritma më fiks. Kjo ndikon në zvogëlimin e numrit të punetorëve nëpër kompani që deri më tani janë marrë me analizim të të dhënave.

Gjatë trajnimit të të dhënave të paetiketuara, çdo shtresë nyjesh mëson atribute në mënyrë automatike, duke tentuar vazhdimisht të rindërtojë hyrjen që e marrin nga mostrat, dhe duke tentuar të zvoglojnë diferencën në mes të supozimeve të rrjetës dhe shpërndarjes së probabilitetit të të dhënave në hyrje. Në këtë proces, këto rrjete mësojnë të njohin korrelacione në mes të atributeve relevante dhe rezultateve optimale.

Avantazhi i këtyre rrjeteve është se mund të përdorim një rrjetë të trajnuar në të dhëna të etiketuara, për të testuar një bashkësi tjetër të të dhënave të paetiketuara. Në këtë mënyrë, arrihet performancë më e lartë: sa më shumë të dhëna që kemi në fazën e trajnimit, aq më shumë shance për saktësi më të lartë ka modeli. Aftësia e deep learning për të përpunuar sasi shumë të mëdha të të dhënave të paetiketuara është një avantazh krahasuar me algoritmat e tjerë.



## Çka është Machine Learning?


Mësimi Automatik (Machine Learning) është një nga teknologjitë më të reja dhe më tërheqëse në fushën e Shkencave Kompjuterike. Ne çdo ditë përdorim algoritma të cilët bazohen te kjo teknologji. Çdo herë që ne bëmë një kërkim në Google, Bing apo në motorë të tjerë kërkimi habitemi nga shpejtësia dhe saktësia e rezultateve. Një nga arsyet është fakti se këta motorë përdorin algoritma të mësimit automatik të zhvilluar nga kompanitë e mëdha si Google e Microsoft. Keni pyetur ndonjëherë se pse dhe si kutia e mesazheve Junk/Spam është e mbushur me emaile që me të vërtetë janë jo të besueshme? Kjo gjithashtu është mundësuar nga këta algoritma.

Esenca e mësimit automatik është fakti që këta algoritma të jenë duke mësuar gjatë gjithë kohës, e jo të mbesin në gjendjen fillestare. Këtu mund ta heqim një paralele me njeriun: ne mësojmë gjatë gjithë kohës, qoftë nga reagimet apo nga rezultatet që shohim.

Arthur Lee Samuel ishte pioner në fushën e lojërave kompjuterike the inteligjences artificiale. Ai ka dhënë një përkufizim mbi “Machine Learning” duke e quajtur fushë studimi që i jep kompjuterit aftesine e të mësuarit pa qenë i programuar në mënyrë ekplicite. Pra, nuk është e nevojshme ta programoni makinën duke i treguar si të mësojë, makina do të gjejë cilsitë më të rëndësishme dhe do zgjedhë rrugën më të mirë të mësimit.

Një përkufizim tjetër nga Tom Mitchell, është pak më i kuptueshëm. Ai jep këtë përkufizim “Një program kompjuterik është duke mësuar nga eksperienca E në një detyrë T matur me performancën P, vetëm nëse performanca P në detyrën T përmirsohet gjate eksperiencës E”.

Mësimi Automatik tenton që duke i ushqyer kompjuterat me të dhëna dhe infromacione në formë të vëzhgimit dhe të bashkëveprimit me botën reale, t’i aftësoj këta kompjutera të mësojnë ashtu siç mëson njeriu, duke u përmirsuar përgjatë të mësuarit në mënyrë autonome.

Terma nga Mësimi Automatik

Disa terma që do të përdoren më poshtë janë:

1. Bashkësia e të dhënave — është një koleksion i të dhënave të ndërlidhura i përbërë nga elemente të ndara por që nga algoritmat e mësimit automatik trajtohet si një njësi e vetme.

2. Të dhënat trajnuese — Një pjesë (zakonisht pjesa më e madhe) e një bashkësie të dhënash që përdoret për të trajnuar kompjuterin për të mësuar diçka. Pra, pjesa e të dhënave që ja japim kompjuterit për ta trajnuar.

3. Të dhënat testuese — Një përqindje më e vogel e të dhënave, që përdoret për të testuar algoritmat dhe aftësine e tyre për të mësuar nga të dhënat trajnuese

Shembull: Në lëndën e matematikës, profesori i lëndës ka një libër me probleme të zgjidhura. Bashkë me profesorin e lëndës kemi zgjidhur 80% të problemeve, mirëpo ai nuk na ka dhënë 20% të problemeve duke thënë se pjesa tjetër e problemeve do të jetë në provim. Profesori përdor këtë 20% të problemeve që ne nuk i kemi parë më parë për të testuar se çfarë kemi mësuar nga 80% të problemeve që kemi parë.

Ne edhe pse nuk i kemi parë këto 20 % të problemeve do të jemi të aftë t’i zgjidhim ato në bazë te problemeve që kemi parë apo zgjidhur më parë.

Profesori në këtë rast është programuesi i algoritmit, libri me probleme të zgjidhura të matematikës është bashkësia e të dhënave, 80 % e problemeve të zgjidhura që profesori i jep studentëve janë të dhënat trajnuese ndërsa 20 % e problemeve që i shohim në provim janë të dhënat testuese. Aftësia jonë për të zgjidhur këto probleme (rezultati nga provimi) mund të krahasohet më saktësinë e të mësuarit (learning accuracy).

Llojet e Algoritmave të Mësimit Automatik
Dallohen disa tipe kryesore të algoritmave të mësimit automatik: Mësimi i Mbikëqyrur (Supervised Lerning), Mësimi i Pambikëqyrur (Unsupervised Learning), Mësimi i Gjysmë-mbykqyrur (Semi Supervised Learning) dhe Mësimi i Përforcuar (Reinfocment Learning).

1. Mësimi i Mbikëqyrur — Këta algoritma aplikohen në të dhëna të reja për të parashikuar ngjarje të së ardhmes duke u bazuar në çka është mësuar më parë duke përdorur etiketat (labels). Duke filluar nga analizat e një bashkësie të dhënash trajnimi, algoritmi prodhon një funksion për të bërë parashikime për rezultatet dalëse. Ky system është i aftë që pas trajnimit të mjaftueshëm të parashikojë kategorinë apo etiketën për çfarëdo të dhënë hyrëse (input data). Nuk përfundon me kaq. Algoritmi, duke parë gabimet që mund të ketë bërë më herët në parashikime krahason këto parashikime me të vërtetën (ground truth), dhe ndryshon modelin në bazë të këtyre mospërputhjeve.

2. Mësimi i Pambikëqyrur — këta algoritma përdoren kur të dhënat trajnuese që ne kemi në dispozicion nuk janë as të klasifikuara e as të etiketuara (labelled). Ky model studion se si një sistem mund të nxjerrë një funksion që të përshkruaj një strukturë të fshehur nga të dhëna të paetiketuara (unlabelled data).

3. Mësimi i Gjysmë-mbykqyrur — këta algoritma janë diku në mes të të parëve dhe të dytëve, pasi përdorin si të dhëna të etiketuara edhe të paetiketuara për të trajnuar modelin. Zakonisht pjesa më e madhe e të dhënave jane te paetiketuara. Këto algoritme janë të afta të përmirsojnë dukshem saktësisne e të mësuarit.

4. Mësimi i Përforcuar — është një metodë e të mësuarit që bashkëvepron me mjedisin duke prodhuar aksione dhe zbulon gabime dhe shpërblime gjatë këtyre bashkëveprimeve. Kërkimi për gabime (errors) dhe shpërblime të vonshme (delayed rewards) janë karakteristikat më të rëndësishme të mësimit të përforcuar. Kjo metodë lejon makinat dhe agjentët softuerikë të përcaktojnë automatikisht sjelljen ideale brenda një konteksti specifik në mënyrë që të maksimizohet performanca e tij. Një shpërblim është i nevojshëm për agjentin të mësojë se cili aksioni është më i miri. Ky reagim quhet sinjali i përforcimit (reinforcement signal).


## Machine Learning me Python

[Please read it on Medium](https://medium.com/@e.shatri1/machine-learning-me-python-1e3006e17381)


## Si funksionon Computer Vision?

Menyra se si njerzit shohin sa eshte komplekse aq eshte edhe mahnitese. Gjithcka filloi milarda vjet me pare, kur disa organizma mikroskopik filluan te zhvillojne disa mutacione qe i ben te ndjeshme ndaj drites. Ne ditet e sotme ekzistojne me qindra mijera organizma qe kane sistem te ngjashem te te parit. Ata kane sy per te kapur driten, pranues (akceptore) ne tru per ta qasur ate drite dhe korteksin vizuel per te procesuar ate. Kjo na mundeson ne te bejme edhe gjerat me te thjeshta sic mund te jete shikimi i perendimit te diellit.

Ne tri dekadat e fundit, shkenctaret filluan nje studim per te zgjeruar kete aftesi tonen, por jo tek vete njeriu por edhe ne kompjutere (makina) poashtu. Kamera e pare u shpik ne vitin 1816, ku nje kuti e vogel mban nje cope leter e ngjyer ne klorid te argjende klorid (silver chloride). Kur shkrepesi (shutter) ishte i hapur, letra do te eeresohej aty ku ishte e ekzposuar drita. Tani 200 vjet me vone, me nje teknologji shume me te avancuar mund te kapim keto fotografi ne forme digjital ne po te njejten kohe te shprepjes. Kjo do te thote se keto aparate jane te gatshme te imitojne se si njerzit kapim driten the ngjyren. Por, sic duket kjo ishte pjesa me e lehte. Sfida e radhes eshte per keto makina te kuptojne se cfare ka ne keto fotografi.


Ne mund te mos e kemi pare kurre me pare kete fotografi te kesaj lule, mirepo truri yne ne baze te fotografive, apo pamjeve qe kemi pare me pare mund ta klasifikoj si lule, nje avantazh tjeter eshte se truri yne ka miliona vjet evulucion qe dergon deri tek prefeksionimi i klasifikimit te objektit ne fotografi si lule. Por kompjuteri nuk e ka kete avantazh, nje kompjuter mund ta shoh kete fotografi si ne foton e meposhte.

Pra makina e sheh si nje vektor te vlerave integjere qe prezantojne intenzitet e ngjyre nga spektrumi. Nese nje njeri lexon kete nuk do te kishte kuptim, eshte vetem nje grup i te dhenave pa kontekst. Por, sic duket konteksti eshte thelbi ne menyre qe algoritmet te jene ne gjendje te kuptojne permbajtjen e imazhit ne menyre te njejte si trusri i njeriut. Per te mundesuar kete ne perdorim nje algoritem qe eshte mjaft i ngjashem me ate qe truri i njeriut perdore per te njejtin operacion, duke perdorur machine learning (lexoni arikullin tim te pare per nje pershkrim te machine learning). Machine learning na lejon ne qe te trajnojnme (train) efektivisht kontekstin per nje set te te dhenave (data set), ashtu qe algoritmi mund te kuptoje se cfare gjithe keta integjer ne nje organizim specifik prezantojne, ne rastin tone nje lule.

Tani le te mendojme, cfare nese kopjuterit i prezantojme fotografi qe njeriu ka veshtiresi te dalloje objektin. A eshte e mundur qe machine learning te arrije rezultate me te mira. Besoj se shumica e keni pare kete fotografi ne internet, dhe jeni munduar te dalloni se cili eshte qeni e cila eshte shtupë dyshemesh.


Me modelin e machine learning mund te marrim qindra fotografi te qeneve perkates dhe shtupave te dyshemes, dhe per aq kohe sa ne e ushqejme me te dhena te mjaftueshme, modeli evetualisht do te jete ne gjendje te tregoje dallimin ne mes te ketyre dy objeketeve.


Vizioni kompjuterik vazhdimisht merr sfida komplekse, dhe po tenton qe saktesia e ketyre algoritmeve te jete e njejte me ate te njeriut. Por, ashtu si njeriu keto modele nuk jane prefekte, ato po ashtu bejne gabime.

### CNN
Nje tip specifik i Neural Networks (NN, shqip: rrjeti nervor) qe mund te arrije saktesi mjaft te mire eshte Convolutional Neural Networks (CNN). CNN fillimisht e copton kete fotografi ne grupe me te vogla te pikseleve qe quhen filter. Cdo filter eshte nje matrice e pikselave, dhe rrjeti (network) ben nje seri kalkulimesh ne keto piksele duke i krahasuar ato kundrejt nje patterni (shqip: model) specifik qe rrjeti po kerkon. Ky rrjet permban nje numer te caktuar te shtresave. Ne shtresen e pare, CNN eshte ne gjendje te detektoje paterne te nivelit me te larte, sic jane skajet e ashpera (rough edges) dhe lakoret. Kur rrjeti performon me shume konvulucione, CNN mund te identifikoje objekte specifike sic jane fetyrat dhe kafshet.

Pyetja eshte, si e di CNN cfare duhet kerkuar ne imazh dhe sa i sakte eshte saktesia e parashikimit (prediction accuracy). Kjo behet e mundshme duke pasur nje sasi te mjaftueshme te te dhenave me etiketa (labels) per fazen e trajnimit. Kur CNN fillon punen, te gjitha vlerat e filterave jane te caktuar ne menyre te rastesishme. Si rezultat, parashikimet iniciale nuk kane shume kuptim. Sa here qe CNN ben nje parashikim kundrejt te dhenave te etiketuara (labeled data), e shfrytezon nje funksion te gabimit (error function) per te krahasuar sa afer ishte parashikimi me etiketen e vertete te te dhenave. Bazuar ne kete gabim CNN do te perditesoj vlerat e filtrave dhe perserit kete proces prape. Idealisht pas cdo perseritje CNN do te performoj me mire, dhe saktesia e parashikimit do te permirsohet.

Tani le te shikojme si do te ishte ky proces nese ne vend te nje fotografie te vetme analizojme nje video me machine learning. Ne parim, nje video eshte vetem nje seri e fotografive. Per te analizuar nje video ne mund te ndertojme nje CNN per te analizuar fotografi. Ne fotografi te palevizshme, ne mund ta perdorim CNN per te identifikuar karakteristika (features). Por kur kalojme ne video, kjo do te jete me e veshtire, meqenese objektet qe duam t’i identifikojme jane ne levizje dhe ndryshojne me kalimin e kohes. Ose, do te kete kontekst ne mes te dy fotografive te njepasnjeshme ne nje video qe eshte shume me rendesi per etiketim. Marrim shembull, nese shohim nje fotografi te nje kutie me libra, ne nuk jemi te sigurt nese duhet t’a etiketojme si “duke paketuar” apo “duke shpaketuar”, per te ditur kete duhet te shohim fotografite para ose pas kesaj fotografie.


Pikerisht ky eshte problemi ku CNN ka veshtiresi. CNN mund te merr parasysh vetem karakteristika hapesinore, te dhenat vizuele ne nje fotografi, por nuk muun te perballoje karakteristike kohe, pra si eshte e lidhur fotografia e tashme me ate te meparshme. Per te adresuar kete problem, ne duhet marre daljen (output) nga CNN dhe t’a ushqejme me te nje model tjeter qe mund te perballoje natyren temporale te viedos.

### RNN

Ky tip i modeleve quhet Recurrent Neural Network (RNN, shqip: rrjet nervor i perseritur). Perderisa CNN i trajton grupet e pikseleve pavaresisht nga grupet tjera, RNN mund ta mbaje informacionin mbi ate se cka eshte procesuar tashme dhe e perdor ate informacion ne marrjen e vendimeve. RNN mund te perballoje shume tipe te te dhenave ne hyrje dhe dalje. Ne rastin tone, kemi nje video te paketimit te nje kutie, RNN merr nje sekuence te fotografive te etiketuar si me poshte, kuti e zbrazur, kuti e hapur dhe ne fund kuti e mbyllur, bazuar ne te tri keto etiketa mund t’a etiketoj kete video si “duke paketuar nje kuti”.


Njejte si CNN, edhe RNN perdor nje funksion te humbjes ose gabimit per te krahasuar parashikimin ne dalje me etiketen e vertete. Me pastaj i rregullon dhe pershtat peshat dhe e ri-proceson kete sekuence te fotografive, deri sa te arrij saktesi me te mire parashikimi.

Sfida e ketyre qasjeve ndaj fotografise dhe videos per te imituar vizonin e njeriut mbetet kerkesa e nje sasie shume e madhe e te dhenave.





[back](./)
